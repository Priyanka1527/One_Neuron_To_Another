{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Ecoli data set :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title of the data set: Protein Localization Sites\n",
    "\n",
    "#### Number of instance: 336\n",
    "\n",
    "#### Number of attributes : 8 (7 predictive, 1 name)\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "  1.  **Sequence Name:** Accession number for the SWISS-PROT database\n",
    "  2.  **mcg:** McGeoch's method for signal sequence recognition.\n",
    "  3.  **gvh:** von Heijne's method for signal sequence recognition.\n",
    "  4.  **lip:** von Heijne's Signal Peptidase II consensus sequence score.\n",
    "  5.  **chg:** Presence of charge on N-terminus of predicted lipoproteins.\n",
    "  6.  **aac:** score of discriminant analysis of the amino acid content of outer membrane and periplasmic proteins.\n",
    "  7.  **alm1:** score of the ALOM membrane spanning region prediction program.\n",
    "  8.  **alm2:** score of ALOM program after excluding putative cleavable signal regions from the sequence.\n",
    "  9.  **Class Distribution:** The class is the localization site.\n",
    "                              - cp  (cytoplasm)                                   \n",
    "                              - im  (inner membrane without signal sequence)                    \n",
    "                              - pp  (perisplasm)                                   \n",
    "                              - imU (inner membrane, uncleavable signal sequence)   \n",
    "                              - om  (outer membrane)                                \n",
    "                              - omL (outer membrane lipoprotein)                \n",
    "                              - imL (inner membrane lipoprotein)                    \n",
    "                              - imS (inner membrane, cleavable signal sequence)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Ecoli.csv', header=None, names=['SequenceName','MCG',\n",
    "                                                                   'GVH','LIP','CHG','AAC','ALM1','ALM2','Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SequenceName</th>\n",
       "      <th>MCG</th>\n",
       "      <th>GVH</th>\n",
       "      <th>LIP</th>\n",
       "      <th>CHG</th>\n",
       "      <th>AAC</th>\n",
       "      <th>ALM1</th>\n",
       "      <th>ALM2</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CRL_ECOLI</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>cp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>MALZ_ECOLI</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.54</td>\n",
       "      <td>cp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>OSMY_ECOLI</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.20</td>\n",
       "      <td>pp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>NFRA_ECOLI</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>om</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>OMPA_ECOLI</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>om</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SequenceName   MCG   GVH   LIP  CHG   AAC  ALM1  ALM2 Class\n",
       "20     CRL_ECOLI  0.40  0.45  0.48  0.5  0.38  0.22  0.00    cp\n",
       "62    MALZ_ECOLI  0.36  0.41  0.48  0.5  0.48  0.47  0.54    cp\n",
       "316   OSMY_ECOLI  0.64  0.66  0.48  0.5  0.41  0.39  0.20    pp\n",
       "262   NFRA_ECOLI  0.61  0.75  0.48  0.5  0.51  0.33  0.33    om\n",
       "264   OMPA_ECOLI  0.74  0.90  0.48  0.5  0.57  0.53  0.29    om"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49, 0.29, 0.48, 0.5 , 0.56, 0.24, 0.35],\n",
       "       [0.07, 0.4 , 0.48, 0.5 , 0.54, 0.35, 0.44],\n",
       "       [0.56, 0.4 , 0.48, 0.5 , 0.49, 0.37, 0.46],\n",
       "       [0.59, 0.49, 0.48, 0.5 , 0.52, 0.45, 0.36],\n",
       "       [0.23, 0.32, 0.48, 0.5 , 0.55, 0.25, 0.35]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare the input data\n",
    "X = df[['MCG', 'GVH', 'LIP', 'CHG', 'AAC', 'ALM1', 'ALM2']]\n",
    "X = np.array(X)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using one-hot-encoding technique to map the categorical value of species to numerical i.e. \n",
    "#(cp, im, pp, imU, om, omL, imL, imS) to (0,1,2,3,4,5,6,7) and then to one-hot encoded \n",
    "#[1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0] and so on.\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "Y = df.Class\n",
    "Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1, 1))\n",
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data set into train/validation/test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeight(nodes):\n",
    "    layers, weights = len(nodes), []\n",
    "    \n",
    "    for i in range(1, layers):\n",
    "        wt = [[np.random.uniform(-1, 1) for k in range(nodes[i-1] + 1)] for j in range(nodes[i])]\n",
    "        weights.append(np.matrix(wt))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralNet(X_train, Y_train, X_val=None, Y_val=None, iterations=10, nodes=[], rate=0.15):\n",
    "    hiddenLayers = len(nodes) - 1\n",
    "    weights = initializeWeight(nodes)\n",
    "\n",
    "    for iteration in range(1, iterations+1):\n",
    "        weights = trainNetwork(X_train, Y_train, rate, weights)\n",
    "\n",
    "        #Print the accuracy of training and validation after every 20 iterations\n",
    "        if(iteration % 20 == 0):\n",
    "            print(\"Iteration {}\".format(iteration))\n",
    "            print(\"Training Accuracy:{}\".format(accuracy(X_train, Y_train, weights)))\n",
    "            if X_val.any():\n",
    "                print(\"Validation Accuracy:{}\".format(accuracy(X_val, Y_val, weights)))\n",
    "                        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(x, weights, layers):\n",
    "    output, current_input = [x], x\n",
    "\n",
    "    for j in range(layers):\n",
    "        activation = Sigmoid(np.dot(current_input, weights[j].T))\n",
    "        output.append(activation)\n",
    "        current_input = np.append(1, activation) # add the bias = 1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropagation(y, output, weights, layers):\n",
    "    outputFinal = output[-1]\n",
    "    error = np.matrix(y - outputFinal) #Calculate the error at last output\n",
    "    \n",
    "    #Back propagate the error\n",
    "    for j in range(layers, 0, -1):\n",
    "        currOutput = output[j]\n",
    "        \n",
    "        if(j > 1):\n",
    "            # Add previous output\n",
    "            prevOutput = np.append(1, output[j-1])\n",
    "        else:\n",
    "            prevOutput = output[0]\n",
    "        \n",
    "        delta = np.multiply(error, sigmoidDerivative(currOutput))\n",
    "        weights[j-1] += rate * np.multiply(delta.T, prevOutput)\n",
    "\n",
    "        wt = np.delete(weights[j-1], [0], axis=1) # Remove bias from weights\n",
    "        error = np.dot(delta, wt) # Calculate error for current layer\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will perform forward and backward propagation, the new weights will be returned n the end\n",
    "def trainNetwork(X, Y, rate, weights):\n",
    "    layers = len(weights)\n",
    "    for i in range(len(X)):\n",
    "        x, y = X[i], Y[i]\n",
    "        x = np.matrix(np.append(1, x)) # Add feature vector\n",
    "        \n",
    "        output = feedForward(x, weights, layers)\n",
    "        weights = backPropagation(y, output, weights, layers)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return np.multiply(x, 1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(item, weights):\n",
    "    layers = len(weights)\n",
    "    item = np.append(1, item)\n",
    "    \n",
    "    #forward propagation\n",
    "    output = feedForward(item, weights, layers)\n",
    "    \n",
    "    outputFinal = output[-1].A1\n",
    "    index = findMaxActivation(outputFinal)\n",
    "\n",
    "    y = [0 for i in range(len(outputFinal))]\n",
    "    y[index] = 1\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxActivation(output):\n",
    "    m, index = output[0], 0\n",
    "    for i in range(1, len(output)):\n",
    "        if(output[i] > m):\n",
    "            m, index = output[i], i\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, weights):\n",
    "    correct_classification = 0\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        x, y = X[i], list(Y[i])\n",
    "        prediction = predict(x, weights)\n",
    "\n",
    "        if(y == prediction):\n",
    "            correct_classification += 1\n",
    "\n",
    "    return correct_classification / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20\n",
      "Training Accuracy:0.63671875\n",
      "Validation Accuracy:0.6206896551724138\n",
      "Iteration 40\n",
      "Training Accuracy:0.68359375\n",
      "Validation Accuracy:0.7586206896551724\n",
      "Iteration 60\n",
      "Training Accuracy:0.7734375\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 80\n",
      "Training Accuracy:0.78515625\n",
      "Validation Accuracy:0.7931034482758621\n",
      "Iteration 100\n",
      "Training Accuracy:0.78125\n",
      "Validation Accuracy:0.7931034482758621\n",
      "Iteration 120\n",
      "Training Accuracy:0.78125\n",
      "Validation Accuracy:0.7931034482758621\n",
      "Iteration 140\n",
      "Training Accuracy:0.78515625\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 160\n",
      "Training Accuracy:0.82421875\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 180\n",
      "Training Accuracy:0.84375\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 200\n",
      "Training Accuracy:0.84765625\n",
      "Validation Accuracy:0.8620689655172413\n",
      "Iteration 220\n",
      "Training Accuracy:0.859375\n",
      "Validation Accuracy:0.8620689655172413\n",
      "Iteration 240\n",
      "Training Accuracy:0.8671875\n",
      "Validation Accuracy:0.8620689655172413\n",
      "Iteration 260\n",
      "Training Accuracy:0.87109375\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 280\n",
      "Training Accuracy:0.87109375\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 300\n",
      "Training Accuracy:0.8828125\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 320\n",
      "Training Accuracy:0.8828125\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 340\n",
      "Training Accuracy:0.88671875\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 360\n",
      "Training Accuracy:0.890625\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 380\n",
      "Training Accuracy:0.890625\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 400\n",
      "Training Accuracy:0.890625\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 420\n",
      "Training Accuracy:0.890625\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 440\n",
      "Training Accuracy:0.8984375\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 460\n",
      "Training Accuracy:0.8984375\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 480\n",
      "Training Accuracy:0.8984375\n",
      "Validation Accuracy:0.8275862068965517\n",
      "Iteration 500\n",
      "Training Accuracy:0.89453125\n",
      "Validation Accuracy:0.8275862068965517\n"
     ]
    }
   ],
   "source": [
    "# Run it here\n",
    "features = len(X[0]) # Number of features - using all of the features except for the sequence name\n",
    "classes = len(Y[0]) # Number of classes\n",
    "\n",
    "layers = [features, 5, 10, classes]\n",
    "rate, iterations = 0.15, 500\n",
    "\n",
    "weights = neuralNet(X_train, Y_train, X_val, Y_val, iterations=iterations, nodes=layers, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "#Final testing accuracy\n",
    "print(\"Testing Accuracy: {}\".format(accuracy(X_test, Y_test, weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check an example prediction !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.37 0.48 0.5  0.43 0.26 0.36] [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[0], list(Y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(predict(X_test[0], weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same class was predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of iterations :\n",
    "\n",
    "    - We had to train the network for upto 500 iterations in case of this data set compared to Iris to get a better \n",
    "    accuracy. We stopeed at 500 in order to avoid overfitting. However, on further trial, it seemed to have gone a \n",
    "    little further as well without getting overfited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
